<!doctype html><html lang=zh-ch><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><title>PySpark分析 | 蛙二的思考</title><meta property="og:title" content="PySpark分析 - 蛙二的思考"><meta property="og:type" content="article"><meta property="article:published_time" content="2021-07-21T12:00:00+08:00"><meta property="article:modified_time" content="2021-07-21T12:00:00+08:00"><meta name=Keywords content><meta name=description content="PySpark分析"><meta name=author content><meta property="og:url" content="/post/210721/"><link rel="shortcut icon" href=/favicon.ico type=image/x-icon><link rel=stylesheet href=/css/normalize.css><link rel=stylesheet href=/css/style.css><script type=text/javascript src=//cdn.bootcdn.net/ajax/libs/jquery/3.4.1/jquery.min.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><a id=logo href>蛙二的思考</a></div><div><nav id=nav-menu class=clearfix><a class=current href>首页</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>PySpark分析</h1></header><date class="post-meta meta-date">2021年7月21日</date><div class=post-content><h2 id=执行过程>执行过程</h2><p>常用的有local和yarn两种模式，写代码或调错阶段，无特殊情况用local，速度快很多。</p><p>pyspark和scala的spark不同在于，某些情况下数据会从jvm回传给py，这个回传的过程是怎么样的？首先，Spark会先把所有py文件放到此次任务driver端所在的节点，比如我的环境放在 /yarn/nodemanager/usercache/xxx/appcache/application_xx/container_xx_01/main.py 目录，启动py的命令是<code>path/bin/python main.py --arg=xx</code>。同时spark会在driver放一个pyspark.zip，解决Py与spark集群通信的问题。driver端任务运行一段时间后，如果发现计算需要把数据传递给executor上的python，就会启动<code>path/bin/python -m pyspark.daemon</code>，没有额外的参数。pyspark.daemon会fork一个进程，然后在子进程里执行pyspark.worker.main函数，数据读写的源头也改为来自socket。实际代码中先会做dup，把socket复制出来提高效率。driver和executor之间通过环境变量和socket传递数据和代码（似乎是pickle序列化），此时的executor会在container_xx_02或03目录内执行。</p><p>进入py代码后，先构建SparkContext对象，构建过程会查找并执行<code>spark-submit pyspark-shell</code>命令，构建一个java的gateway，再通过Py4J包，以类似RPC的方式把py代码通过Gateway发送到jvm，进行spark操作。如果计算过程中需要python的udf，则数据必须发送到work节点，过程是由spark启动python的worker.py进程，并以环境变量的方式把端口告知worker，worker会用socket去连接这个port，并做一系列判断，比如driver和worker的python版本必须一致，计算结束后再用socket发送回spark。理论上只要数据不回传给py，开销只是方法的传递，性能和scala的实现是一样的，如果有数据回传，速度会降低一倍以上。</p><h2 id=pyspark内容>PySpark内容</h2><h3 id=包层次>包层次</h3><p>顶层目录pyspark包含SparkConf、SparkContext、RDD等spark的基础概念，包含sql、streaming、ml、mllib等多个子模块。</p><h3 id=流程和关键概念>流程和关键概念</h3><p>如果是写类SQL功能，流程是套路化的</p><ol><li>获取SparkConf，设置master和appName。我只用过yarn模式</li><li>把Conf作为参数传给SparkContext。注意，必须构造context，否则无法和spark通信。Conf可以没有，但考虑要设置的参数很多，用Conf方便，另外还有序列化类参数可传入，默认用pickle序列化py和jvm之间的数据</li><li>通过Context来获取SparkSession。这个Session是属于pyspark.sql的类，整合了SQLContext和HiveContext等多个SQL会用到的功能</li></ol><p>拿到SparkSession后，读取文件得到的数据呈现形式就是DataFrame类，这个类具备很多SQL语义的API（因为Session就是sql包下的一个类）。DataFrame可以链式操作，即操作后返回的值大部分情况下仍是DataFrame，如果做了groupBy操作，得到的是GroupedData类型。</p><h3 id=pyspark命令>pyspark命令</h3><p>执行这个命令，会自动加载shell.py脚本并初始化sc(pyspark.context), spark(pyspark.sql.session，对应原生SparkSession类), sql(spark.sql的别名), sqlCtx/sqlContext(pyspark.sql.context.SQLContext)共4个全局变量。</p></div><div class="post-meta meta-tags">没有标签</div></article></div><footer id=footer><div>&copy; 2022 <a href>蛙二的思考 By</a></div><br><div><div class=github-badge><a href=https://gohugo.io/ target=_black rel=nofollow><span class=badge-subject>Powered by</span><span class="badge-value bg-blue">Hugo</span></a></div><div class=github-badge><a href=https://www.flysnow.org/ target=_black><span class=badge-subject>Design by</span><span class="badge-value bg-brightgreen">蛙二</span></a></div><div class=github-badge><a href=https://github.com/flysnow-org/maupassant-hugo target=_black><span class=badge-subject>Theme</span><span class="badge-value bg-yellowgreen">Maupassant</span></a></div></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[["$","$"]],processEscapes:!0}}</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script>
<a id=rocket href=#top></a>
<script type=text/javascript src="/js/totop.js?v=0.0.0" async></script></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=Search>
<input type=hidden name=sitesearch>
<button type=submit class="submit icon-search"></button></form></section><section class=widget><h3 class=widget-title>最近文章</h3><ul class=widget-list><li><a href=/post/220421/ title=数据引擎和算子对比>数据引擎和算子对比</a></li><li><a href=/post/220405/ title=Kubernetes初学笔记>Kubernetes初学笔记</a></li><li><a href=/post/220312/ title=分布式计算在Spark上的实现>分布式计算在Spark上的实现</a></li><li><a href=/post/220204/ title=分布式哈希技术摘录>分布式哈希技术摘录</a></li><li><a href=/post/220201/ title=压缩技术浅谈>压缩技术浅谈</a></li><li><a href=/post/220115/ title=Python的数据科学相关库介绍>Python的数据科学相关库介绍</a></li><li><a href=/post/211226/ title=SU的执行过程与用户登陆机制>SU的执行过程与用户登陆机制</a></li><li><a href=/post/211222/ title=SQL的JOIN种类与选择>SQL的JOIN种类与选择</a></li><li><a href=/post/211218/ title=内存使用的观察和理解>内存使用的观察和理解</a></li><li><a href=/post/211115/ title=搭建最小化的Linux系统>搭建最小化的Linux系统</a></li></ul></section><section class=widget><h3 class=widget-title><a href=/categories/>分类</a></h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title><a href=/tags/>标签</a></h3><div class=tagcloud></div></section><section class=widget><h3 class=widget-title>其它</h3><ul class=widget-list><li><a href=index.xml>文章 RSS</a></li></ul></section></div></div></div></div></body></html>