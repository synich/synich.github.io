<!doctype html><html lang=zh-ch><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><title>神经网络来源与分类 | 蛙二的思考</title><meta property="og:title" content="神经网络来源与分类 - 蛙二的思考"><meta property="og:type" content="article"><meta property="article:published_time" content="2020-07-27T12:00:00+08:00"><meta property="article:modified_time" content="2020-07-27T12:00:00+08:00"><meta name=Keywords content><meta name=description content="神经网络来源与分类"><meta name=author content><meta property="og:url" content="/post/200727/"><link rel="shortcut icon" href=/favicon.ico type=image/x-icon><link rel=stylesheet href=/css/normalize.css><link rel=stylesheet href=/css/style.css><script type=text/javascript src=//cdn.bootcdn.net/ajax/libs/jquery/3.4.1/jquery.min.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><a id=logo href>蛙二的思考</a></div><div><nav id=nav-menu class=clearfix><a class=current href>首页</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>神经网络来源与分类</h1></header><date class="post-meta meta-date">2020年7月27日</date><div class=post-content><h2 id=早期历史>早期历史</h2><p>神经网络技术起源于上世纪五、六十年代，当时叫感知机（perceptron），包含有输入层、输出层和一个隐藏层。输入的特征向量通过隐藏层变换到达输出层，由输出层得到分类结果。但早期的单层感知机存在一个严重的问题——它对稍微复杂一些的函数都无能为力（如异或操作）。直到上世纪八十年代才被Hinton、Rumelhart等人发明的多层感知机克服，就是具有多层隐藏层的感知机。</p><p>多层感知机可以摆脱早期离散传输函数的束缚，使用sigmoid或tanh等连续函数模拟神经元对激励的响应，在训练算法上则使用Werbos发明的反向传播BP算法。这就是现在所说的神经网络NN。</p><p>神经网络的层数直接决定了它对现实的刻画能力——利用每层更少的神经元拟合更加复杂的函数。但问题出现了——随着神经网络层数的加深，优化函数越来越容易陷入局部最优解，并且这个“陷阱”越来越偏离真正的全局最优。利用有限数据训练的深层网络，性能还不如较浅层网络。同时，另一个不可忽略的问题是随着网络层数增加，“梯度消失”现象更加严重。（具体来说，我们常常使用sigmoid作为神经元的输入输出函数。对于幅度为1的信号，在BP反向传播梯度时，每传递一层，梯度衰减为原来的0.25。层数一多，梯度指数衰减后低层基本上接受不到有效的训练信号。）</p><h2 id=大发展与大突破>大发展与大突破</h2><p>在2010年代的后半程，图像和文本领域分别产生了堪称革命性的ResNet（深度残差网络）和Bert模型。</p><p>图像领域一直沿着CNN的方向发展，最早取得突破的是Yann LeCun在1998年提出的LeNet，在32x32的小图片效果有突破，但不能处理大图片。但毕竟奠定了现代卷积神经网络的原型，即卷积，池化，全链接。</p><p>在这之后CNN的锋芒开始被SVM等手工设计的特征盖过。随着ReLU和dropout的提出，以及GPU和大数据带来的历史机遇，CNN在2012年迎来了历史突破，这一年的ImageNet上AlexNet一举夺冠，开启了神经网络识图的时代。但AlexNet没有在方法论上给出方向，之后的VGG使用一系列大小为3x3的小尺寸卷积核和pooling层构造深度卷积神经网络，并取得了较好的效果。</p><p>2014年的ImageNet冠军是GoogLeNet，它的主要特点是网络不仅有深度，还在横向上具有“宽度”。2015年ImageNet的冠军ResNet，更是将图像分类识别错误率降低到了3.6%，超过了正常人眼识别的精度。</p><p>而文本领域由于前后相关性，起初都是基于RNN在做，但RNN存在串行缺陷，很难并行。文本领域的突破稍晚于图像，2017年Google提出Transform，这个模型由Encoder-Decoder组成，它的Self-Attention和Position Embedding可以替代RNN来做Seq2Seq任务。Attention是个精妙的词法袋（由Yoshua Bengio提出），但不能识别位置，配合上Position Embedding就完整地解决了机器翻译的问题。</p><p>2018年出现的Bert也是以Transformer为基础，但只使用Decoder部分，因此只有Self-Attention，没有普通的Attention。</p><p>2006年Hinton最早提出深度学习的概念，具体是利用预训练的方式缓解了局部最优解的问题，将隐藏层增加到了7层，实现了真正意义上的“深度”。LeCun、Hinton和Bengio一起因深度学习上的贡献获得了2018年图灵奖。</p><h2 id=各种网络的简单比较>各种网络的简单比较</h2><p>对主要的3种分类归纳如下：</p><ol><li>DNN：为了克服梯度消失，ReLU、maxout等传输函数代替了sigmoid，形成了如今DNN的基本形式。结构跟多层感知机一样。</li><li>RNN：DNN无法对时间序列上的变化进行建模，但时间顺序对于自然语言处理、语音识别、手写体识别等应用非常重要。为了适应这种需求，就出现了循环神经网络RNN。</li><li>CNN：图像中存在固有的局部模式（如人脸中的眼睛、鼻子、嘴巴等），所以将图像处理和神将网络结合引出卷积神经网络CNN。CNN是通过卷积核将上下层进行链接，同一个卷积核在所有图像中是共享的，图像通过卷积操作后仍然保留原先的位置关系。</li></ol><p>在普通的全连接网络或CNN中，每层神经元的信号只能向上一层传播，样本的处理在各个时刻独立，因此又被成为前向神经网络(Feed-forward Neural Networks)。而在RNN中，神经元的输出可以在下一个时间段直接作用到自身，即第i层神经元在m时刻的输入，除了(i-1)层神经元在该时刻的输出外，还包括其自身在(m-1)时刻的输出！表示成图就是这样的：</p><p>为方便分析，按照时间段展开如下图所示：</p><p>（t+1）时刻网络的最终结果O（t+1）是该时刻输入和所有历史共同作用的结果！这就达到了对时间序列建模的目的。RNN可以看成一个在时间上传递的神经网络，它的深度是时间的长度!正如我们上面所说，“梯度消失”现象又要出现了，只不过这次发生在时间轴上。</p><p>所以RNN存在无法解决长时依赖的问题。为解决上述问题，提出了LSTM（长短时记忆单元），通过cell门开关实现时间上的记忆功能，并防止梯度消失。在序列信号分析中，如果能预知未来，对识别一定也是有所帮助的。因此就有了双向RNN、双向LSTM，同时利用历史和未来的信息。</p><p>事实上，不论是哪种网络，他们在实际应用中常常都混合着使用，比如CNN和RNN在上层输出之前往往会接上全连接层，很难说某个网络到底属于哪个类别。不难想象随着深度学习热度的延续，更灵活的组合方式、更多的网络结构将被发展出来。</p><p>简单总结如下：
﻿
CNN(卷积神经网络)、RNN(循环神经网络)、DNN(深度神经网络)的内部网络结构的区别</p><p>先说DNN，从结构上来说他和传统意义上的NN（神经网络）没什么区别，但是神经网络发展时遇到了一些瓶颈问题。一开始的神经元不能表示异或运算，科学家通过增加网络层数，增加隐藏层可以表达。并发现神经网络的层数直接决定了它对现实的表达能力。但是随着层数的增加会出现局部函数越来越容易出现局部最优解的现象，用数据训练深层网络有时候还不如浅层网络，并会出现梯度消失的问题。</p><p>CNN与RNN的比较</p><p>相同点</p><ol><li>传统神经网络的扩展。</li><li>前向计算产生结果，反向计算模型更新。</li><li>每层神经网络横向可以多个神经元共存,纵向可以有多层神经网络连接。</li></ol><p>不同点</p><ol><li>CNN空间扩展，神经元与特征卷积；RNN时间扩展，神经元与多个时间输出计算</li><li>RNN可以用于描述时间上连续状态的输出，有记忆功能，CNN用于静态输出</li><li>CNN可以达到1000+深度，RNN深度有限</li></ol></div><div class="post-meta meta-tags">没有标签</div></article></div><footer id=footer><div>&copy; 2022 <a href>蛙二的思考 By</a></div><br><div><div class=github-badge><a href=https://gohugo.io/ target=_black rel=nofollow><span class=badge-subject>Powered by</span><span class="badge-value bg-blue">Hugo</span></a></div><div class=github-badge><a href=https://www.flysnow.org/ target=_black><span class=badge-subject>Design by</span><span class="badge-value bg-brightgreen">蛙二</span></a></div><div class=github-badge><a href=https://github.com/flysnow-org/maupassant-hugo target=_black><span class=badge-subject>Theme</span><span class="badge-value bg-yellowgreen">Maupassant</span></a></div></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[["$","$"]],processEscapes:!0}}</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script>
<a id=rocket href=#top></a>
<script type=text/javascript src="/js/totop.js?v=0.0.0" async></script></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=Search>
<input type=hidden name=sitesearch>
<button type=submit class="submit icon-search"></button></form></section><section class=widget><h3 class=widget-title>最近文章</h3><ul class=widget-list><li><a href=/post/220421/ title=数据引擎和算子对比>数据引擎和算子对比</a></li><li><a href=/post/220405/ title=Kubernetes初学笔记>Kubernetes初学笔记</a></li><li><a href=/post/220312/ title=分布式计算在Spark上的实现>分布式计算在Spark上的实现</a></li><li><a href=/post/220204/ title=分布式哈希技术摘录>分布式哈希技术摘录</a></li><li><a href=/post/220201/ title=压缩技术浅谈>压缩技术浅谈</a></li><li><a href=/post/220115/ title=Python的数据科学相关库介绍>Python的数据科学相关库介绍</a></li><li><a href=/post/211226/ title=SU的执行过程与用户登陆机制>SU的执行过程与用户登陆机制</a></li><li><a href=/post/211222/ title=SQL的JOIN种类与选择>SQL的JOIN种类与选择</a></li><li><a href=/post/211218/ title=内存使用的观察和理解>内存使用的观察和理解</a></li><li><a href=/post/211115/ title=搭建最小化的Linux系统>搭建最小化的Linux系统</a></li></ul></section><section class=widget><h3 class=widget-title><a href=/categories/>分类</a></h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title><a href=/tags/>标签</a></h3><div class=tagcloud></div></section><section class=widget><h3 class=widget-title>其它</h3><ul class=widget-list><li><a href=index.xml>文章 RSS</a></li></ul></section></div></div></div></div></body></html>